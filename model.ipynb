{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg2pVCu85aUG",
        "outputId": "35eab327-8449-4f88-90a2-69267773f991"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 225,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# importing packages\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acrQb_mb53wY",
        "outputId": "66dbc85b-9b9b-4584-a798-b1f92f5b5bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "8302\n"
          ]
        }
      ],
      "source": [
        "print(torch.backends.cudnn.is_available())\n",
        "print(torch.backends.cudnn.version())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ9c4MjL5aUI"
      },
      "source": [
        "## Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "i5QEPQq_5aUK"
      },
      "outputs": [],
      "source": [
        "f = open('sentiment labelled sentences/imdb_labelled.txt','r').readlines() + open('sentiment labelled sentences/yelp_labelled.txt','r').readlines() + open('sentiment labelled sentences/amazon_cells_labelled.txt','r').readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHGGzt_K5aUK"
      },
      "source": [
        "## Train split test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "7qm9SwDA5aUL"
      },
      "outputs": [],
      "source": [
        "# prepare the training and testing sets\n",
        "x_test_r, y_test = [], []\n",
        "x_train_r, y_train = [], []\n",
        "\n",
        "# Take 10% of the whole dataset for testing\n",
        "for line in f[:300]:\n",
        "    line = line.split('\\t')\n",
        "    # if len(line[0].split(\" \")) < 30 and len(line[0].split(\" \")) > 0:\n",
        "    y_test.append(int(line[-1]))\n",
        "    x_test_r.append(line[0].lower())\n",
        "\n",
        "# Take 90% of the whole dataset for training\n",
        "for line in f[300:]:\n",
        "    line = line.split('\\t')\n",
        "\n",
        "    # For better training, we will eliminate some too long sentences and too short sentences. Here, I put all sentences < 30\n",
        "    # if len(line[0].split(\" \")) < 30 and len(line[0].split(\" \")) > 0:\n",
        "    y_train.append(int(line[-1]))\n",
        "    x_train_r.append(line[0].lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "fGfU5_Lc5aUL"
      },
      "outputs": [],
      "source": [
        "# Remove all the special mark from our dataset\n",
        "def clean_mark(li):\n",
        "    char = '\\!|\\@|\\#|\\$|\\%|\\^|\\&|\\*|\\(|\\)|\\_|\\+|\\{|\\}|\\[|\\]|\\;|\\:|\\'|\\\"|\\<|\\>|\\,|\\.|\\/|\\?|\\`|\\~|\\-'\n",
        "\n",
        "    formatted = []\n",
        "\n",
        "    # replace the special char with empty strings and strip the sentences\n",
        "    li_char = list(map(lambda x: re.sub(char, \"\", x).strip(), li))\n",
        "\n",
        "    # Split the sentences into words\n",
        "    formatted = list(map(lambda x: [i for i in x.split(\" \") if i != ''], li_char))\n",
        "\n",
        "    return formatted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "FdxDFuGD5aUL"
      },
      "outputs": [],
      "source": [
        "x_train_mk, x_test_mk = clean_mark(x_train_r), clean_mark(x_test_r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uosBe2885aUL"
      },
      "source": [
        "## Vocab and look up table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "hboDNqRR5aUM"
      },
      "outputs": [],
      "source": [
        "# Create the vocab and look up table from the training set, any word that is not in alphabet is set as <ukn>\n",
        "vocab_r = list(set(word if word.isalpha() else \"<ukn>\" for senten in x_train_mk for word in senten))\n",
        "vocab_r.sort()\n",
        "\n",
        "# Put the <pad> into the vocab for padding purpose\n",
        "vocab = ['<pad>'] + vocab_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "vBXmYEvq5aUM"
      },
      "outputs": [],
      "source": [
        "# Create the look up table\n",
        "ind_to_word = {i: vocab[i] for i in range(len(vocab))}\n",
        "word_to_ind = {w: i for i, w in ind_to_word.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC_GJ1Ef5aUM"
      },
      "source": [
        "## Clean up data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "UZDoSMEd5aUM"
      },
      "outputs": [],
      "source": [
        "# Replace all the unknown word with <ukn> in the testing dataset\n",
        "\n",
        "def clean_unk(li, vocab):\n",
        "    for senten in li:\n",
        "        for i in range(len(senten)):\n",
        "            if senten[i] not in vocab:\n",
        "                senten[i] = '<ukn>'\n",
        "\n",
        "    return li\n",
        "\n",
        "x_train_ukn, x_test_ukn = clean_unk(x_train_mk, vocab), clean_unk(x_test_mk, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppaBPsd_5aUM",
        "outputId": "5f4ae52c-72f4-4332-b510-2b32f0308c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max: 70\n",
            "Min: 1\n",
            "Mean: 11.690740740740742\n",
            "Len: 2700\n",
            "\n",
            "Len test: 300\n"
          ]
        }
      ],
      "source": [
        "# This is where I check if the lengths of each sentence in the training dataset do not vary too much. If\n",
        "# then, I can change the condition at the begining of the reading dataset.\n",
        "\n",
        "len_train = []\n",
        "for i in x_train_ukn:\n",
        "    len_train.append(len(i))\n",
        "\n",
        "print(\"Max:\", max(len_train))\n",
        "print(\"Min:\", min(len_train))\n",
        "print(\"Mean:\", sum(len_train)/len(len_train))\n",
        "print(\"Len:\", len(x_train_ukn))\n",
        "print()\n",
        "# x_test_ukn = x_test_ukn[: len(x_train_ukn)//10]\n",
        "print(\"Len test:\", len(x_test_ukn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "aSDH7KYK5aUN"
      },
      "outputs": [],
      "source": [
        "# Padding the datasets, the longest sentence will be the standard length of all sentences.\n",
        "total = x_train_ukn + x_test_ukn\n",
        "\n",
        "max_le = 0\n",
        "for i in total:\n",
        "    if len(i) > max_le:\n",
        "        max_le = len(i)\n",
        "\n",
        "# Add the <pad> to all sentences in both dataset\n",
        "x_train_pad = [x + ['<pad>']*(max_le - len(x)) for x in x_train_ukn]\n",
        "x_test_pad = [x + ['<pad>']*(max_le - len(x)) for x in x_test_ukn]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyk-Fedw5aUN"
      },
      "source": [
        "## Sentence to Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "Hshz_cAe5aUN"
      },
      "outputs": [],
      "source": [
        "## Function to convert word to index using the look up table\n",
        "\n",
        "def sentence_to_ind(li, word_to_ind):\n",
        "    sen_ind = []\n",
        "    for sentence in li:\n",
        "        sen_ind.append( [ word_to_ind[x] for x in sentence ] )\n",
        "\n",
        "    return sen_ind\n",
        "\n",
        "x_train, x_test = sentence_to_ind(x_train_pad, word_to_ind), sentence_to_ind(x_test_pad, word_to_ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9ZDU6Y_5aUN",
        "outputId": "cddb5d98-58c9-4e47-e47f-28fb2319e457"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "execution_count": 237,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(x_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDHNxLj15aUN"
      },
      "source": [
        "## Batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "QVCIjn_U5aUN"
      },
      "outputs": [],
      "source": [
        "# Convert all the training and testing datasets to tensor\n",
        "\n",
        "def convert_to_tensor(x_li, y_li):\n",
        "    tensor_x_li = torch.tensor(x_li)\n",
        "    tensor_y_li = torch.tensor([ int(y) for y in y_li])\n",
        "    return tensor_x_li, tensor_y_li\n",
        "\n",
        "x_train_tensor, y_train_tensor = convert_to_tensor(x_train, y_train)\n",
        "x_test_tensor, y_test_tensor = convert_to_tensor(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "hcvWi4ft5aUN"
      },
      "outputs": [],
      "source": [
        "# Batching the datasets, with the batch size of 128\n",
        "\n",
        "data = list(zip(x_train_tensor, y_train_tensor))\n",
        "batch_size = 128\n",
        "shuffle = True\n",
        "\n",
        "loader = DataLoader(data, batch_size=batch_size, shuffle = shuffle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tugy_Pwe5aUN"
      },
      "source": [
        "## Models implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "qQ5X5AGR5aUN"
      },
      "outputs": [],
      "source": [
        "# Bidirectional LSTM model\n",
        "\n",
        "class modelSentiment(nn.Module):\n",
        "    def __init__(self, n_class, n_hidden):\n",
        "        super(modelSentiment, self).__init__()\n",
        "\n",
        "        # Declare variables\n",
        "        self.n_class = n_class\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embed = nn.Embedding(num_embeddings=n_class, embedding_dim=100)\n",
        "\n",
        "        # LSTM layer, bidirectional = True -> the output will have the dimesion of 2 * n_hidden\n",
        "        self.lstm = nn.LSTM(input_size = 100, hidden_size = n_hidden, bidirectional=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.W = nn.Linear(n_hidden * 2, 2, bias=True)\n",
        "        self.probabilities = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, tensor_train): # batch_size x len_sentence [128, 19]\n",
        "\n",
        "        # Transpose because this allows us to take the last hidden state later\n",
        "        tensor_train = tensor_train.transpose(0, 1) # L x B [19, 128]\n",
        "\n",
        "        # Embedding layer\n",
        "        embed = self.embed(tensor_train) # L x B x E [19, 128, 100]\n",
        "        \n",
        "        # BiLSTM layer\n",
        "        model, (_, _) = self.lstm(embed) # L x B x H [19, 128, 256]\n",
        "\n",
        "        # Take the last hidden state\n",
        "        model = model[-1] # B x H [128, 256]\n",
        "\n",
        "        # Output layer\n",
        "        model = self.W(model) # B x 2 [128, 2]\n",
        "        model = self.probabilities(model) # B x 2 [128, 2]     \n",
        "        \n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "lP2JN2XQ5aUN"
      },
      "outputs": [],
      "source": [
        "# Attention model\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, n_class, n_hidden):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        # Declare variables\n",
        "        self.n_class = n_class\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embed = nn.Embedding(num_embeddings=n_class, embedding_dim=100)\n",
        "\n",
        "        # LSTM layer\n",
        "        # We need two RRN network, one for encoding and one for decoding.\n",
        "        self.enc = nn.LSTM(input_size = 100, hidden_size = n_hidden)\n",
        "        self.dec = nn.LSTM(input_size = n_hidden, hidden_size = 1)\n",
        "\n",
        "        # Softmax layer for the weights\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "        # Output layer\n",
        "        self.W = nn.Linear(n_hidden, 2, bias=True)\n",
        "        self.probabilities = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, tensor_train): # batch_size x len_sentence [128, 19]\n",
        "        \n",
        "        # Embedding layer\n",
        "        embed = self.embed(tensor_train) # B x L x E [128, 19, 100]\n",
        "        \n",
        "        # Pass the last hidden states of the encoding to the decoding to get the weights\n",
        "        enc_outputs, (_, _) = self.enc(embed) # B x L x H [128, 19, 256]\n",
        "        dec_outputs, (_, _) = self.dec(enc_outputs) # B x L x 1 [128, 19, 1]\n",
        "\n",
        "        # Softmax the weights\n",
        "        dec_outputs_sm = torch.tensor([])\n",
        "        dec_outputs_sm = dec_outputs_sm.to(device)\n",
        "\n",
        "        for i in dec_outputs:\n",
        "            dec_outputs_sm = torch.cat( ( dec_outputs_sm, self.sm(i.squeeze(1).unsqueeze(0).to(device)).to(device) ),  0)\n",
        "\n",
        "        dec_outputs_sm = dec_outputs_sm.unsqueeze(1) # B x 1 x L [128, 1, 19]\n",
        "\n",
        "        # Calculate the weight sum\n",
        "        weight_sum = torch.matmul(dec_outputs_sm, enc_outputs) # B x 1 x H [128, 1, 256]\n",
        "\n",
        "        # Output layer\n",
        "        model = self.W(weight_sum) # B x 1 x 2 [128, 1, 2]\n",
        "        model = model.squeeze(1) # B x 2 [128, 2]\n",
        "        model = self.probabilities(model) # B x 2 [128, 2]\n",
        "        \n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s65Er4645aUO"
      },
      "source": [
        "## Training model LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "PFhfdZV_5aUO"
      },
      "outputs": [],
      "source": [
        "# Declare the n_class = number of word in the vocab\n",
        "# Hidden state = 128\n",
        "n_class = len(vocab)\n",
        "n_hidden = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "lL13sgrK5aUO"
      },
      "outputs": [],
      "source": [
        "# Init model LSTM\n",
        "model_lstm = modelSentiment(n_class, n_hidden)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(model_lstm.parameters(), lr=0.0001)\n",
        "\n",
        "# Put the model into the cuda for computatiing purpose on cuda\n",
        "model_lstm = model_lstm.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR1IrlkA5aUO",
        "outputId": "561f3e65-592e-45bc-874d-d4454349e30d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0010 cost = 15.258881\n",
            "Epoch: 0020 cost = 15.244247\n",
            "Epoch: 0030 cost = 15.250197\n",
            "Epoch: 0040 cost = 15.244195\n",
            "Epoch: 0050 cost = 15.242241\n",
            "Epoch: 0060 cost = 15.253512\n",
            "Epoch: 0070 cost = 15.246881\n",
            "Epoch: 0080 cost = 15.242767\n",
            "Epoch: 0090 cost = 15.255420\n",
            "Epoch: 0100 cost = 15.254030\n",
            "Epoch: 0110 cost = 15.246911\n",
            "Epoch: 0120 cost = 15.235857\n",
            "Epoch: 0130 cost = 15.248224\n",
            "Epoch: 0140 cost = 15.244251\n",
            "Epoch: 0150 cost = 15.245834\n",
            "Epoch: 0160 cost = 15.246093\n",
            "Epoch: 0170 cost = 15.256033\n",
            "Epoch: 0180 cost = 15.241031\n",
            "Epoch: 0190 cost = 15.247469\n",
            "Epoch: 0200 cost = 15.126817\n",
            "Epoch: 0210 cost = 15.286882\n",
            "Epoch: 0220 cost = 15.249876\n",
            "Epoch: 0230 cost = 15.244909\n",
            "Epoch: 0240 cost = 15.240001\n",
            "Epoch: 0250 cost = 15.244601\n",
            "Epoch: 0260 cost = 15.244958\n",
            "Epoch: 0270 cost = 15.240298\n",
            "Epoch: 0280 cost = 15.249593\n",
            "Epoch: 0290 cost = 15.251680\n",
            "Epoch: 0300 cost = 15.245340\n",
            "Epoch: 0310 cost = 15.249856\n",
            "Epoch: 0320 cost = 15.255143\n",
            "Epoch: 0330 cost = 15.249961\n",
            "Epoch: 0340 cost = 15.246250\n",
            "Epoch: 0350 cost = 15.243114\n",
            "Epoch: 0360 cost = 15.238592\n",
            "Epoch: 0370 cost = 15.240559\n",
            "Epoch: 0380 cost = 15.232081\n",
            "Epoch: 0390 cost = 15.248035\n",
            "Epoch: 0400 cost = 15.244248\n",
            "Epoch: 0410 cost = 15.227839\n",
            "Epoch: 0420 cost = 15.242788\n",
            "Epoch: 0430 cost = 15.247536\n",
            "Epoch: 0440 cost = 15.245804\n",
            "Epoch: 0450 cost = 15.245001\n",
            "Epoch: 0460 cost = 15.251310\n",
            "Epoch: 0470 cost = 15.241913\n",
            "Epoch: 0480 cost = 15.239401\n",
            "Epoch: 0490 cost = 15.255453\n",
            "Epoch: 0500 cost = 15.241994\n"
          ]
        }
      ],
      "source": [
        "# Train the data for the \"epoch = 500\" times\n",
        "for epoch in range(500):\n",
        "    tot_loss = 0 # calculate the total loss of each batch\n",
        "\n",
        "    for x_train_l, y_train_l in loader:\n",
        "\n",
        "        # Put the batch onto cuda\n",
        "        x_train_l, y_train_l = x_train_l.to(device), y_train_l.to(device)\n",
        "        \n",
        "        # Set the optimizer back to 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Feed the x_train to the model\n",
        "        output  = model_lstm(x_train_l)\n",
        "\n",
        "        # Calculate the loss and backproping\n",
        "        loss = criterion(output, y_train_l.squeeze(0))\n",
        "        loss.backward()\n",
        "    \n",
        "        # Calculate the total loss and optimize\n",
        "        tot_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    # print the total loss\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(tot_loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmau4sVl5aUO"
      },
      "source": [
        "## Training model Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "qtyQqPij5aUO"
      },
      "outputs": [],
      "source": [
        "## Training model Attention\n",
        "# Declare the n_class = number of word in the vocab\n",
        "# Hidden state = 128\n",
        "n_class = len(vocab)\n",
        "n_hidden = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "oGiD3LwP5aUO"
      },
      "outputs": [],
      "source": [
        "# Init model Attention\n",
        "model_attention = Attention(n_class, n_hidden)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(model_attention.parameters(), lr=0.0001)\n",
        "\n",
        "# Put the model into the cuda for computatiing purpose on cuda\n",
        "model_attention = model_attention.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbW9cBvJ5aUO",
        "outputId": "aba859e0-d5ea-4b2f-96d1-55e3e804896f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0010 cost = 15.228873\n",
            "Epoch: 0020 cost = 15.218556\n",
            "Epoch: 0030 cost = 15.117722\n",
            "Epoch: 0040 cost = 14.758621\n",
            "Epoch: 0050 cost = 14.054735\n",
            "Epoch: 0060 cost = 13.288648\n",
            "Epoch: 0070 cost = 12.548198\n",
            "Epoch: 0080 cost = 12.114475\n",
            "Epoch: 0090 cost = 11.619663\n",
            "Epoch: 0100 cost = 11.173743\n",
            "Epoch: 0110 cost = 11.003304\n",
            "Epoch: 0120 cost = 10.546913\n",
            "Epoch: 0130 cost = 10.329826\n",
            "Epoch: 0140 cost = 10.246729\n",
            "Epoch: 0150 cost = 9.959751\n",
            "Epoch: 0160 cost = 9.779602\n",
            "Epoch: 0170 cost = 9.477413\n",
            "Epoch: 0180 cost = 9.363777\n",
            "Epoch: 0190 cost = 9.235867\n",
            "Epoch: 0200 cost = 9.038111\n",
            "Epoch: 0210 cost = 8.882900\n",
            "Epoch: 0220 cost = 8.745993\n",
            "Epoch: 0230 cost = 8.773301\n",
            "Epoch: 0240 cost = 8.598092\n",
            "Epoch: 0250 cost = 8.490643\n",
            "Epoch: 0260 cost = 8.365248\n",
            "Epoch: 0270 cost = 8.357336\n",
            "Epoch: 0280 cost = 8.226789\n",
            "Epoch: 0290 cost = 8.155202\n",
            "Epoch: 0300 cost = 8.076519\n",
            "Epoch: 0310 cost = 8.001636\n",
            "Epoch: 0320 cost = 8.006361\n",
            "Epoch: 0330 cost = 7.949170\n",
            "Epoch: 0340 cost = 8.026319\n",
            "Epoch: 0350 cost = 7.838537\n",
            "Epoch: 0360 cost = 7.798966\n",
            "Epoch: 0370 cost = 7.724132\n",
            "Epoch: 0380 cost = 7.672285\n",
            "Epoch: 0390 cost = 7.635479\n",
            "Epoch: 0400 cost = 7.634457\n",
            "Epoch: 0410 cost = 7.549905\n",
            "Epoch: 0420 cost = 7.506296\n",
            "Epoch: 0430 cost = 7.506413\n",
            "Epoch: 0440 cost = 7.510949\n",
            "Epoch: 0450 cost = 7.535496\n",
            "Epoch: 0460 cost = 7.435149\n",
            "Epoch: 0470 cost = 7.483411\n",
            "Epoch: 0480 cost = 7.409315\n",
            "Epoch: 0490 cost = 7.368941\n",
            "Epoch: 0500 cost = 7.369490\n"
          ]
        }
      ],
      "source": [
        "# Train the data for the \"epoch = 500\" times\n",
        "for epoch in range(500):\n",
        "    tot_loss = 0 # calculate the total loss of each batch\n",
        "\n",
        "    for x_train_a, y_train_a in loader:\n",
        "\n",
        "        # Put the batch onto cuda\n",
        "        x_train_a, y_train_a = x_train_a.to(device), y_train_a.to(device)\n",
        "        \n",
        "        # Set the optimizer back to 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Feed the x_train to the model\n",
        "        output = model_attention(x_train_a)\n",
        "\n",
        "        # Calculate the loss and backproping\n",
        "        loss = criterion(output, y_train_a.squeeze(0))\n",
        "        loss.backward()\n",
        "    \n",
        "        # Calculate the total loss and optimize\n",
        "        tot_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    # print the total loss\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(tot_loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86iEsH0D5aUO"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc6gkNZC9wEJ",
        "outputId": "37160ce5-b4ec-4db2-8a2f-d10429673fd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   2, 4621, 4621,  ...,    0,    0,    0],\n",
              "        [2886, 4173, 4759,  ...,    0,    0,    0],\n",
              "        [   1,    1, 4800,  ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [ 146, 2035, 4133,  ...,    0,    0,    0],\n",
              "        [2117, 1968, 4357,  ...,    0,    0,    0],\n",
              "        [2117,  945, 4299,  ...,    0,    0,    0]], device='cuda:0')"
            ]
          },
          "execution_count": 248,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test_tensor = x_test_tensor.to(device)\n",
        "y_test_tensor = y_test_tensor.to(device)\n",
        "x_test_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbSfhS3q5aUO",
        "outputId": "70e8bb02-1290-4eaf-867a-5cf8879dd6af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  42.333333333333336 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "predict = model_lstm(x_test_tensor).data\n",
        "predict_1 = [int(np.argmax(x)) for i, x in enumerate(predict.cpu())]\n",
        "\n",
        "# Count the number of correct prediction\n",
        "count = 0\n",
        "for i in range(len(x_test_tensor)):\n",
        "    if predict_1[i] == int(y_test_tensor[i]):\n",
        "        count += 1\n",
        "    \n",
        "# Print the result\n",
        "print(\"Accuracy: \", count/len(y_test_tensor)*100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtD2UlnV5aUO",
        "outputId": "fb0aaefe-09b4-431d-908e-792afb7564db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  78.0 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "predict = model_attention(x_test_tensor).data\n",
        "predict_1 = [int(np.argmax(x)) for i, x in enumerate(predict.cpu())]\n",
        "\n",
        "# Count the number of correct prediction\n",
        "count = 0\n",
        "for i in range(len(x_test_tensor)):\n",
        "    if predict_1[i] == y_test_tensor[i]:\n",
        "        count += 1\n",
        "\n",
        "# Print the result\n",
        "print(\"Accuracy: \", count/len(y_test_tensor)*100, \"%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f48d9550db41daf67a6ff456c92b13d24bd6d6b8fa468b40929ebf6b89e0fcf3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
